<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="deep learning," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="本章内容：  前向网络的基本概念 学习XOR的例子 基于梯度下降的学习 隐藏节点 网络结构设计 BP及其他微分算法">
<meta name="keywords" content="deep learning">
<meta property="og:type" content="article">
<meta property="og:title" content="flower book: deep feedforward networks">
<meta property="og:url" content="http://yoursite.com/2018/02/11/flower-book-deep-feedforward-networks/index.html">
<meta property="og:site_name" content="Paul&#39;s Gardon">
<meta property="og:description" content="本章内容：  前向网络的基本概念 学习XOR的例子 基于梯度下降的学习 隐藏节点 网络结构设计 BP及其他微分算法">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2018-03-11T10:55:56.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="flower book: deep feedforward networks">
<meta name="twitter:description" content="本章内容：  前向网络的基本概念 学习XOR的例子 基于梯度下降的学习 隐藏节点 网络结构设计 BP及其他微分算法">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.2',
    sidebar: {"position":"right","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/02/11/flower-book-deep-feedforward-networks/"/>





  <title>flower book: deep feedforward networks | Paul's Gardon</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Paul's Gardon</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/02/11/flower-book-deep-feedforward-networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ApostlePaul">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Paul's Gardon">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">flower book: deep feedforward networks</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-02-11T14:38:09+08:00">
                2018-02-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/reading-notes/" itemprop="url" rel="index">
                    <span itemprop="name">reading notes</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o">本文总阅读量</i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>次
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>本章内容：</p>
<ol>
<li>前向网络的基本概念</li>
<li>学习XOR的例子</li>
<li>基于梯度下降的学习</li>
<li>隐藏节点</li>
<li>网络结构设计</li>
<li>BP及其他微分算法</li>
</ol>
<a id="more"></a>
<p>深度前向网络（deep feedforward networks）的目标是要近似一些函数f*。<br>之所以叫“前向传播”是因为网络没有反馈到自身的链接，有的话就是RNN了。<br>需要注意的是，虽然叫神经网络，但是前向网络的目标是实现统计上的泛化，而非模拟大脑的功能。<br>可以从线性模型出发来理解前向网络及其动机:<br>像线性回归或逻辑回归这样的线性模型的局限性在于容量有限，因此我们可以用x的非线性方程来扩展，把x变为$\phi (x)$，这个$\phi (x)$可以被认为是x的一个新表征，或者描述x的一种特征。<br>所以问题变为怎么来选取映射$\phi$：</p>
<ol>
<li>如果是特别通用的$\phi$，容量是够了，但是容易过拟合，而且不能纳入(encode)一些有效的先验信息。</li>
<li>如果是人工选取的特异化的$\phi$，那么在相应的topic上会有较好的表现，（并且在deep Learning出现之后，这种方式是占主导地位的。）但是这样需要很高的人力成本。</li>
</ol>
<p>deep learning的模型很好的解决了以上两个问题。我们可以认为我们的模型是：</p>
<script type="math/tex; mode=display">
y = f(x;\theta ,\omega) = \phi(x;\theta)^T\omega</script><p>这样可以用$\theta$从比较广的函数集合中学习$\phi$，再用参数$\omega$把$\phi(x)$映射到期望的输出。<br>从这个角度来看，隐层其实就是$\phi(x)$</p>
<h3 id="1-Example-Learning-XOR"><a href="#1-Example-Learning-XOR" class="headerlink" title="1. Example: Learning XOR"></a>1. Example: Learning XOR</h3><p>一个求解异或的例子。<br>训练集的输入为四个点：</p>
<script type="math/tex; mode=display">
X = {[0,0]^T, [0,1]^T, [1,0]^T, [1,1]^T}</script><p>输出为：</p>
<script type="math/tex; mode=display">
Y = {0, 1, 1, 0}</script><p>选取的loss function为MSE(在此只是为了简化数学计算，通常并不用这个。)：</p>
<script type="math/tex; mode=display">
J(\theta) = 1/4 \Sigma (f^* (x) - f(x;\theta ))^2</script><p>如果选取$f(x;\theta )$为线性模型的话：</p>
<script type="math/tex; mode=display">
f(x;\theta ) = f(x;\omega,b) = x^T \omega + b</script><p>最小化$J(\theta)$会算出$\omega=0, b=1/2$。所有输出值都是0.5，无解。<br>所以引入一个隐层，先将输入进行一次放射变换，即非线性的变换(这里用的是ReLU)，再进行线性求解。</p>
<script type="math/tex; mode=display">
h = f^(1)(x;\Omega,c)
y = f^(2)(h;\omega,b)
f(x, \Omega, c, \omega, b) = \omega^T max{0, \Omega^T x + c} + b</script><p>对此模型，我们可以直接设计出模型中的参数满足求解。但是在大量数据，有噪声，非理想的情况下，就要用梯度下降求解了。</p>
<h3 id="2-Gradient-Based-Learning"><a href="#2-Gradient-Based-Learning" class="headerlink" title="2. Gradient-Based Learning"></a>2. Gradient-Based Learning</h3><p>求解神经网络和求解参数化的模型其实很像，最大的区别就在于神经网络的loss function通常是非凸的，因此我们通常求不出全局最优解，而是局部的低值。<br>凸优化可以保证收敛，即，对于所有的初始值，凸优化的算法都会收敛到极值点。但是非凸的神经网络保证不了，并且其还对初始值很敏感。</p>
<h4 id="2-1-cost-functions"><a href="#2-1-cost-functions" class="headerlink" title="2.1 cost functions"></a>2.1 cost functions</h4><p>通常，参数模型会定义一个分布$p(y;x,\theta)$，然后运用最大似然的原则，即，用的是训练数据和模型预估结果直接的交叉熵cross-entropy。<br>神经网络和这个很相似，并且有时候也会只预估y的一些统计量而非整个分布，以及也会加正则项。</p>
<h5 id="2-1-1-用最大似然学条件概率分布"><a href="#2-1-1-用最大似然学条件概率分布" class="headerlink" title="2.1.1 用最大似然学条件概率分布"></a>2.1.1 用最大似然学条件概率分布</h5><p>大部分的神经网络都会用最大似然来训练，也就是说，cost function是负对数似然函数。一方面，负对数似然函数跟交叉熵是等价的，另一方面，负对数似然可以防止saturate。</p>
<script type="math/tex; mode=display">
J(\theta) = -E_{x,y ~ \hat{p}_{data}}log p_{model}(y|x)</script><p>其中$log p<em>{model}$的形式决定了代价函数的形式。例如$p</em>{model}(y|x)$是高斯分布的话，代价函数跟均方误差（MSE）是等价的。<br>使用最大似然的优点：</p>
<ol>
<li>明确了$p(y|x)$，则代价函数也随之确定了。</li>
<li>防止饱和。例如很多输出单元都会包含一个指数函数，这是这些单元的变量取绝对值非常大的负数时，会饱和。负对数似然函数会消除掉这些指数效果。</li>
</ol>
<p>缺点是：</p>
<ol>
<li>负对数似然没有一个最小值，如果输出是离散值，那么其输出不了0或1，只能无限接近。</li>
<li>对于实值的输出变量，如果模型控制输出分布的概率密度，那么对于正确的训练集可能赋予极高的概率密度，导致交叉熵(-logp)趋近于负无穷。正则技术可以用来解决这个问题。</li>
</ol>
<h5 id="2-1-2-学习条件统计量"><a href="#2-1-2-学习条件统计量" class="headerlink" title="2.1.2 学习条件统计量"></a>2.1.2 学习条件统计量</h5><p>given x, 我们经常只需要学习y的统计量。例如只预估y的均值。<br>这时，我们可以把神经网络看做是一类函数f的集合，这类函数只受特征连续性及各种bound的限制，而非一组特定参数。也既，从泛函的角度来看，我们可以设计cost functional在某些我们想要的特点函数上有最小值。解泛函需要用到变分，后面在详细讨论，现在只需要知道两个结果。</p>
<ol>
<li>如果我们可以从真实分布中获取无穷多的样本，那么最小化MSE，我们可以得到这样的函数：given x, 输出y的平均值。</li>
<li>同样，最小化MAE, 我们可以得到这样的函数：given x, 输出y的中位数。<br>但是，我们用基于梯度的优化，在MSE和MAE上通常会得到比较差的结果，因为输出节点很容易saturate。所以尽管有时候不用预估y的整个分布，我们还是会用cross-entropy。</li>
</ol>
<h4 id="2-2-output-Units"><a href="#2-2-output-Units" class="headerlink" title="2.2 output Units"></a>2.2 output Units</h4><p>ouput unit的形式决定了cross-entropy的形式。</p>
<h5 id="2-2-1-输出高斯分布用线性单元"><a href="#2-2-1-输出高斯分布用线性单元" class="headerlink" title="2.2.1 输出高斯分布用线性单元"></a>2.2.1 输出高斯分布用线性单元</h5><p>线性单元的形式是：给定特征h，输出向量</p>
<script type="math/tex; mode=display">
\hat{y} = W^T h+b</script><p>线性输出单元经常用于产出条件高斯分布的均值。这时最大化对数似然跟最小化MSE等价。但是求解过程要求高斯分布的协方差矩阵为正定矩阵，这很难。所以其他的输出节点会被使用，见2.2.4。<br>但是线性单元的一个好处是不会saturate，所以它们使用基于梯度的优化算法很方便。</p>
<h5 id="2-2-2-输出伯努利分布用sigmoid单元"><a href="#2-2-2-输出伯努利分布用sigmoid单元" class="headerlink" title="2.2.2 输出伯努利分布用sigmoid单元"></a>2.2.2 输出伯努利分布用sigmoid单元</h5><p>二值问题的最大似然是要定义一个x条件下的关于y的伯努利分布。<br>所以神经网络需要预估$P(y=1|x)$, 输出值要在[0, 1]区间内。<br>这时用线性单元是不行的。原因如下：<br>假设我们依据线性单元设计如下的输出形式：</p>
<script type="math/tex; mode=display">
P(y=1|x) = max{0, min{1, \omega^Th+b}}</script><p>这时当$\omega^Th+b$落在[0,1]区间之外时，其梯度为0，学习算法不在知道在这样的情况下如何更新参数。<br>而我们需要一种单元，无论何时预估错误，都可以得到一个强梯度。sigmoid节点就是这样的，其形式为：</p>
<script type="math/tex; mode=display">
\hat{y} = \sigma(\omega^Th + b)</script><p>这种在对数空间里预测概率的方法可以可以很自然地用最大似然学习，因为最大似然的代价函数是$-logP(y|x)$，其中的log可以抵消sigmoid中的exp。<br>（书中讲了为什么对于sigmoid单元用最大似然的loss function很好，不会saturate, 而MSE不好，这里先不展开。）<br>需要注意对数运算时underflow的问题。在软件实现时，经常将损失函数写作关于z的函数，而非$\hat{y}=\sigma(z)$的函数。</p>
<h5 id="2-2-3-输出多维伯努利分布用softmax单元"><a href="#2-2-3-输出多维伯努利分布用softmax单元" class="headerlink" title="2.2.3 输出多维伯努利分布用softmax单元"></a>2.2.3 输出多维伯努利分布用softmax单元</h5><p>需要表征n个不同类型的概率分布的时候，可以用softmax。可以把softmax看做是sigmoid的扩展。<br>对于二分类问题，我们希望模型产出一个单一数值：</p>
<script type="math/tex; mode=display">
\hat{y}=P(y=1|x)</script><p>考虑到这个数值是在0/1之间的，并且我们希望得到这个数值的对数以方便进行基于梯度的对数似然优化，所以我们实际上预估的是$z = log \hat{P}{y=1 | x}$，从而我们得到一个被sigmoid函数控制的伯努利分布。<br>这一思路同样可以运用在softmax上：<br>首先，我们得到一个非归一的log probabilities:</p>
<script type="math/tex; mode=display">
z = W^Th+b</script><p>其中$z_i = log \hat{P}(y=i|x)$<br>然后我们对其进行指数化和归一化，得到新的z，即softmax(z):</p>
<script type="math/tex; mode=display">
{softmax(z)}_i= exp(z_i)/\Sigma_j exp(z_j)</script><p>再演变成最大化目标$log P(y=i;z) = log {softmax(z)}_i$。<br>不用log的目标函数不好，因为没有办法抵消掉exp，会导致梯度消失。<br>softmax也会saturate，就像sigmoid会在输入在正负无穷处会saturate一样。softmax会在输入的n个值之间差别特别大的时候saturate。</p>
<h5 id="2-2-4-其他输出单元"><a href="#2-2-4-其他输出单元" class="headerlink" title="2.2.4 其他输出单元"></a>2.2.4 其他输出单元</h5><p>以上的线性、simoid和softmax的输出单元是最常见的。不过最大似然的原则和性质为我们设计任意类型的输出单元提供了指导。<br>一般来讲，如果我们定义了一个条件概率分布$p(y|x;\theta)$，最大似然原则建议我们用$-logp(y|x;\theta)$最为代价函数。我们可以把神经网络看做是一个函数$f(x;\theta)$，这个函数的输出不直接是预估值y，而是关于y分布的参数，即$f(x;\theta) = \omega$，这是代价函数可以表示成$-logp(y;\omega(x))$。<br>（书中在后续讲了异方差模型和混合密度网络两个例子。）</p>
<h3 id="3-Hidden-Units"><a href="#3-Hidden-Units" class="headerlink" title="3. Hidden Units"></a>3. Hidden Units</h3><p>本节讨论如何选择隐藏单元，这方面的研究还很活跃，目前还没有确定的理论指导原则。<br>一些隐藏节点并不是处处可微的，例如ReLU，在0点是不可微的。但是其使用的实际效果还是很好的。一方面，是因为NN的学习算法并不一定要达到局部最小值点，更重要的是要有明显的梯度下降，所以代价函数的最小值对应于梯度未定义的点是可以接受的。另外ReLU在0点左侧和右侧都有导数，只是左右导数不相等。而计算机在实际计算的过程中，输入真正为0的可能性不大，通常都是0值左右一个非常小的数值。所以在实践中我们可以放心地使用ReLU。<br>默认情况下，隐藏单元都是以一个向量$\bf{x}$为输入，经过仿射变换$z = W^Tx+b$，然后利用函数$g(z)$对$z$进行逐个元素的变换。通常我们只以$g(z)$来区分不同的隐层单元，前面仿射变换的过程都是一样的。</p>
<h4 id="3-1-Rectified-Linear-Units-and-Their-Generalizations"><a href="#3-1-Rectified-Linear-Units-and-Their-Generalizations" class="headerlink" title="3.1 Rectified Linear Units and Their Generalizations"></a>3.1 Rectified Linear Units and Their Generalizations</h4><p>ReLU的公式：</p>
<script type="math/tex; mode=display">
g(z) = max\{0, z\}.</script><p>ReLU易于优化，因为其处于激活状态时（输入大于0），其导数很大且一致，并且二阶导数几乎处处为0，这意味着，相比于引入二阶影响的激活函数，其梯度方向对于学习来说更有用。<br>因为$h=g(W^Tx + b)$，所以在初始化的时候，把b设置成一个比较小的正值，如0.1，是比较好的，以此来使隐藏单元在初始化使尽量处于被激活的状态。<br>ReLU的一个缺点是在激活为0的时候不能通过梯度的方式进行学习。所有一些泛化的ReLU针对此缺点进行了改进。另$g(z)=max(0,z)+\alpha_i min(0,z)$。则当$\alpha = -1$时，就可以得到绝对值整流（Absolute value rectification）。当$\alpha = 0.01$这样的比较小的值时，就可以得到Leaky ReLU。如果把$\alpha$当做一个可学习的参数，那么就得到参数化ReLU(PReLU)。<br>Maxout Units是对ReLU的进一步泛化。maxout不是作用于每个元素的函数，而是把z划分为不同的组，每个组有k个值，输出每组中最大的值：</p>
<script type="math/tex; mode=display">
g(z)_{i} = max_{j \in G^{(i)}} z_j</script><p>Maxout单元可以学习具有多达k段的分段线性的凸函数。maxout单元因此可以视为学习激活函数本身而不仅仅是单元之间的关系。如果k足够大，那么maxout单元可以以任意的精度来近似任何凸函数。如果是两段的，那么maxout可以学出来跟传统的ReLU(leaky ReLU，PReLU, AReLU)一样的函数（怎么实现的？）。不过学习的机理是不一样的。<br>每个Maxout单元由k维权重向量来参数化，因此maxout需要更多的正则化。<br>另外Maxout可以使下一层减少k倍的权重，对抗灾难遗忘（catastrophic forgetting）。<br>ReLU及其泛化的一个原则是：如果他们的行为更接近线性，那么模型更容易优化。这一原则不仅在深度线性网络上适用。</p>
<h4 id="3-2-Logistic-Sigmoid-and-Hyperbolic-Tangent"><a href="#3-2-Logistic-Sigmoid-and-Hyperbolic-Tangent" class="headerlink" title="3.2 Logistic Sigmoid and Hyperbolic Tangent"></a>3.2 Logistic Sigmoid and Hyperbolic Tangent</h4><p>在ReLU之前，用的比较多的激活函数是sigmoid函数和双曲正切函数：</p>
<script type="math/tex; mode=display">
g(z) = \sigma(z)
g(z) = tanh(z)</script><p>这两个很像，因为$tanh(z) = 2\sigmoid(2z) - 1$。<br>sigmoid函数在高值和低值的时候都会饱和，只有在0附近才对梯度很敏感。这不利于基于梯度的学习。而他们被用着输出单元是因为最大似然的log会抵消掉exp。<br>如果一定要用sigmoid函数，那么双曲正切函数会更好一点，因为它在0附近更像单位函数。<br>sigmoid函数在非前向网络用的更多，像RNN，概率模型，和一些自编码器有一些额外的需求使得分段线性激活函数不能用，这时候sigmoid函数更有吸引力。</p>
<h4 id="3-3-Other-Hidden-Units"><a href="#3-3-Other-Hidden-Units" class="headerlink" title="3.3 Other Hidden Units"></a>3.3 Other Hidden Units</h4><p>一般来说，许多可微方程都可以表现的挺好，不过跟现在的也都差不多，所以只有比较大的提升的激活函数才会引起兴趣。<br>需要特别注意的有以下几种。</p>
<ol>
<li>不要激活函数，也即用单位函数作为激活函数。例如，对于变换$h = g(W^Tx + b)$，如果可以接收W是低秩的，那么可以通过把一层线性层分解层两个单位函数层，来实现$W^T = V^TU^T$的分解，从而减少参数。</li>
<li>softmax也可以作为隐藏单元。会用着比较高级的结构中，如学习去操作记忆。</li>
<li>RBF，$h<em>i = exp(-1/\delta^2_i||W</em>{:,i} - x||^2)$，只在x接近$W_{:,i}$时才活跃，在其他地方都饱和，所以很难优化。</li>
<li>Softplus，ReLU的光滑版本，处处可微，但是实际效果并没有ReLU好。</li>
<li>Hard Tanh，$g(a) = max(-1, min(1, a))$。</li>
</ol>
<h3 id="4-Architecture-Design"><a href="#4-Architecture-Design" class="headerlink" title="4. Architecture Design"></a>4. Architecture Design</h3><p>网络的结构描述了网络有多少单元以及这些单元是怎样相互连接的。<br>大多数的网络结构会把不同的节点组合成层，层之间是链式关系。<br>这些链式的结构需要考虑网络的层深以及每层的宽度。一般来讲，网络约深，需要用到的参数越少，但是也越难优化。</p>
<h4 id="4-1-Universal-Approximation-Properties-and-Depth"><a href="#4-1-Universal-Approximation-Properties-and-Depth" class="headerlink" title="4.1 Universal Approximation Properties and Depth"></a>4.1 Universal Approximation Properties and Depth</h4><p>万能近似定理（universal approximation theorem）表明，一个前向网络，如果它有一个线性输出层和至少一个有“挤压”激活方程的隐层，给它足够的节点数，那么它可以以任意小的误差近似有限维空间中的Borel可测函数。且网络的倒数也可以任意好地近似。<br>（在这里我们只需要知道所有定义在封闭有界n维实数集子集上的连续函数都是Borel可测的，也是可以被前向网络近似的。）<br>有限维的离散空间的函数也可以用前向网络近似。ReLU也可以。<br>万能近似定理表明了无论我们试图学习什么函数，一个足够大的MLP一定可以表示这个函数，但是学习算法不一定能学得到。原因有二：</p>
<ol>
<li>用于训练的优化算法可能找不到期望函数相应的参数值。</li>
<li>训练算法可能由于过拟合而选择了错误的函数。<br>另一方面，我们不知道这个网络到底会有多大。<br>所以，使用更深的模型能够减少表示期望函数所需的单元的数量，并且可以减少泛化误差。<br>我们还可能出于统计原因选择深度模型。因为我们选择一个模型的时候，我们隐含地考虑进去了一些先验的信念。一个普遍的信念是我们学习的函数由几个更加简单的函数组合而成。对应于计算机程序的几个步骤，其中每个步骤使用前一步骤的输出。</li>
</ol>
<h4 id="4-2-Other-Architecture-Considerations"><a href="#4-2-Other-Architecture-Considerations" class="headerlink" title="4.2 Other Architecture Considerations"></a>4.2 Other Architecture Considerations</h4><p>在实践当中，神经网络显示出相当的多样性。<br>许多神经网络已被开发用于特定的任务。计算机视觉使用CNN，涉及到序列处理的使用RNN。<br>另外还有层与层之间进行跳跃链接。</p>
<h3 id="5-Back-Propagation-and-Other-Diﬀerentiation-Algorithms"><a href="#5-Back-Propagation-and-Other-Diﬀerentiation-Algorithms" class="headerlink" title="5. Back-Propagation and Other Diﬀerentiation Algorithms"></a>5. Back-Propagation and Other Diﬀerentiation Algorithms</h3><p>前向网络，从输入$x$开始，向前进行信息传播，得到网络的输出$\hat{y}$。反向传播，从代价函数$J(\theta)$开始，向后传播误差信息，用于计算网络参数的梯度。<br>注意BP(反向传播)是描述梯度计算的方法，而不是整个NN的学习算法，SGD这样的才是。并且BP也不仅仅适用于多层神经网络。<br>计算图及链式法则在此不再介绍。<br>前向传播及反向传播的过程，网上已有很多文章介绍，并且我可以手推，在此不再介绍。</p>
<h4 id="5-1-Symbol-to-Symbol-Derivatives"><a href="#5-1-Symbol-to-Symbol-Derivatives" class="headerlink" title="5.1 Symbol-to-Symbol Derivatives"></a>5.1 Symbol-to-Symbol Derivatives</h4><p>计算图是符号描述，当我们使用或者训练NN时，需要给这些符号赋上具体的值。<br>一些BP的方法，输入的是计算图和输入数值，直接返回参数在这些输入数值处梯度的具体值。这种方法叫符号到数值的微分（symbol-to-number differentiation）。Torch和Caffe用的就是这种方法。<br>另外一种方法，输入计算图，程序会给计算图增加一些节点，这些节点是需要计算的导数的符号描述。Theano和TensorFlow用的就是这样的方法。这种方法的优点是导数可以使用与原始表达式相同的语言来描述。因为导数计算只是另外一张计算图。如果需要计算更高阶的导数，我们只需要再次运行计算图。</p>
<h4 id="5-2-General-Back-Propagation"><a href="#5-2-General-Back-Propagation" class="headerlink" title="5.2 General Back-Propagation"></a>5.2 General Back-Propagation</h4><hr>
<p>Algorithm 1. The outermost skeleton of the bp algorithm。This portion does simple setup and cleanup work. Most of the important work happens in the build_grad subroutine of algorithm 2</p>
<hr>
<p>Require: T, the target set of variables whose gradients must be computed.<br>Require: G, the computational graph<br>Require: z, the variable to be differentiated<br>  Let $G^’$ be G pruned to contain only nodes that are ancestors of z and descendents of nodes in T.<br>  Initialize grad_table, a data structure associating tensors to their gradients</p>
<script type="math/tex; mode=display">
  grad_table[z] \leftarrow 1$
  for V in T do
    build_grad(V,G,G^',grad_table)
  end for</script><p>  return grad_table restricted to T.</p>
<hr>
<hr>
<p>Algorithm 2. The inner loop subroutine build_grad(V,${\rm G}$, ${\rm G}’$,grad_table) of bp, called by algorithm 1.</p>
<hr>
<p>if V is in grad_table then<br>  return grad_table[V]<br>end if</p>
<p>$$<br>i \leftarrow 1<br>for C in get_consumers(V, {\rm G}’) do<br>  op \leftarrow get_operation(C)<br>  D \leftarrow build_grad(C, {\rm G}, {\rm G}’, grad_table)<br>  G^{(i)} \leftarrow op.bprop(get_inputs(C, {rm G}’), V, D)<br>  i \leftarrow i + 1<br>end for<br>G \leftarrow \Sigma_iG^{(i)}<br>grad_table[V] = G<br>insert G and the operations creating it into {\rm G}<br>return G</p>
<hr>
<h4 id="5-3-Differentiation-outside-the-Deep-Learning-Community"><a href="#5-3-Differentiation-outside-the-Deep-Learning-Community" class="headerlink" title="5.3 Differentiation outside the Deep Learning Community"></a>5.3 Differentiation outside the Deep Learning Community</h4><p>反向传播只是自动微分的一种方法，它是一种称为反向累加模式（reverse mode accumlation）的特例。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/deep-learning/" rel="tag"># deep learning</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/12/17/my-usage-of-protobuf/" rel="next" title="my usage of protobuf">
                <i class="fa fa-chevron-left"></i> my usage of protobuf
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/02/28/ML-treasures/" rel="prev" title="ML treasures">
                ML treasures <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="ApostlePaul" />
          <p class="site-author-name" itemprop="name">ApostlePaul</p>
           
              <p class="site-description motion-element" itemprop="description">天地渺渺一尘子<br/>随风而起<br/>随雨而落<br/>随遇而安</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
            
              <a href="/archives/">
            
                <span class="site-state-item-count">24</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">2</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">9</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Example-Learning-XOR"><span class="nav-text">1. Example: Learning XOR</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Gradient-Based-Learning"><span class="nav-text">2. Gradient-Based Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-cost-functions"><span class="nav-text">2.1 cost functions</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#2-1-1-用最大似然学条件概率分布"><span class="nav-text">2.1.1 用最大似然学条件概率分布</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-1-2-学习条件统计量"><span class="nav-text">2.1.2 学习条件统计量</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-output-Units"><span class="nav-text">2.2 output Units</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#2-2-1-输出高斯分布用线性单元"><span class="nav-text">2.2.1 输出高斯分布用线性单元</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-2-2-输出伯努利分布用sigmoid单元"><span class="nav-text">2.2.2 输出伯努利分布用sigmoid单元</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-2-3-输出多维伯努利分布用softmax单元"><span class="nav-text">2.2.3 输出多维伯努利分布用softmax单元</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-2-4-其他输出单元"><span class="nav-text">2.2.4 其他输出单元</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Hidden-Units"><span class="nav-text">3. Hidden Units</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-Rectified-Linear-Units-and-Their-Generalizations"><span class="nav-text">3.1 Rectified Linear Units and Their Generalizations</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-Logistic-Sigmoid-and-Hyperbolic-Tangent"><span class="nav-text">3.2 Logistic Sigmoid and Hyperbolic Tangent</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-Other-Hidden-Units"><span class="nav-text">3.3 Other Hidden Units</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Architecture-Design"><span class="nav-text">4. Architecture Design</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-Universal-Approximation-Properties-and-Depth"><span class="nav-text">4.1 Universal Approximation Properties and Depth</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-Other-Architecture-Considerations"><span class="nav-text">4.2 Other Architecture Considerations</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-Back-Propagation-and-Other-Diﬀerentiation-Algorithms"><span class="nav-text">5. Back-Propagation and Other Diﬀerentiation Algorithms</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-Symbol-to-Symbol-Derivatives"><span class="nav-text">5.1 Symbol-to-Symbol Derivatives</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-General-Back-Propagation"><span class="nav-text">5.2 General Back-Propagation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-3-Differentiation-outside-the-Deep-Learning-Community"><span class="nav-text">5.3 Differentiation outside the Deep Learning Community</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ApostlePaul</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动</div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">主题 &mdash; <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.2</div>


        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user">本站访客数</i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人次
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye">本站总访问量</i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.2"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  

    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://yoursite.com/2018/02/11/flower-book-deep-feedforward-networks/';
          this.page.identifier = '2018/02/11/flower-book-deep-feedforward-networks/';
          this.page.title = 'flower book: deep feedforward networks';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://paul.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  










  





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
